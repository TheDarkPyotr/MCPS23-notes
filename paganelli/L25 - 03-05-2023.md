# Theory of signals
Theory of signals allows *sensors* to sample real signals or physical quantities in a finite set of time (*like temperatures*) and transform them into *discrete, digital signals* (*in a computable format of finite set values*), processing them and transmitting the obtained *discrete result*.
Under *wireless transmission*, the theory of signals introduce the tools to transform a data packet into an *analog signal* (*often radio waves*) and receive analog radio signals, transofrming them into *digital signals*.

To understand these steps, it's necessary to understand the nature of signals, deriving how they can be *decomposed, reconstruncted and sampled*. All of this activities passes through the understanding of **Fourier series**.

## Classification of signals
We consider only **deterministic signals** because are known before they are produced, differently from *random* signals that are analyzed using probabilsitic methods.
A **deterministic signal** can be represented by a function $f(t)$ of a real value (*generally the time*):
									$f(t): \mathbb{D} \rightarrow \mathbb{E}$
The domain and codomain can be either the *set of real number $\mathbb{R}$*  or a *discrete set* $\mathbb{Z}$, or even the *set of complex numbers $\mathbb{C}$.* The set of complex number $\mathbb{C}$ allows to represent two independent signals combined together.

So, a *signal* is a real function of time as: 
- **Time-continuous and amplitude-continuous** (*analog signals*): if the domain $\mathbb{D}$ and the codomain $\mathbb{E}$ are the set of real numbers $\mathbb{R}$ the signal is charaterized by ***continous amplitude*** .
- ![[Pasted image 20230503163054.png]]
- **Time-continuos and quantized**  (*quantized signals*):  if  if the domain $\mathbb{D}$ is  the set of real numbers $\mathbb{R}$ and the co-domain $\mathbb{E}$ is the discrete set $\mathbb{Z}$, the signal is charaterized by ***discrete amplitude*** (*or quantized signal*)
	![[Pasted image 20230503163114.png]]
- **Time-discrete and amplitude-continuous** (*discrete signals*): the domain is the *set of integers $\mathbb{Z(T)} = \{nT, \forall n \in \mathbb{Z}, T \in \mathbb{R}\}$*. In the pictured example, $\mathbb{Z(2)} = \{..,-4,-2,0,2,4\}$. 
- ![[Pasted image 20230503163148.png]]
- **Time-discrete and quantized** (*digital signals*): a discrete signal is known as **digital signal** (*or symbolic sequence*) when the codomain is a *finite set of symbols*. For example, a text is an example of symbolic signal. 
![[Pasted image 20230503163203.png]]

#### Digital signals example
Consider the alphabet of 26 symbols $A = \{a,b,c,..,x,y,z\}$. A symbolic signal is any sequenc eof such symbol like $ababfxje$. 

Differently, if consider the alphabet of two symbols $B = \{0,1\}$, we can represent any symbol in $A$ with a sequence of symbols in $B$ like $a = 0000, b = 0001$, etc. 
Thus, the digital signal $ababfxje$ would become $0000000001000000000100101101110100100100$.

Now, assume a source able to transmit $f$ symbols/second (*symbol frequency*), having:
- 8 symbols with the alphabet $A$: the transmission last $\frac{8}{f}$ seconds.
- 40 symbols with alphabet $B$: the transmission last $\frac{40}{f}$ seconds
The term *throughput* usually refers to the *binary frequency*. Assume a source that samples an analog signal with $f_{c}$ samples/seconds, each sample is represented (***quantized***) with $M$ bit/sample thus the source has a **throughput of $f_{c}\times M$ bit/second. 

##### Question
Considering a source that emits a digital signal encoded in an alphabet of 8 symbols, with a symbol frequency equal to 10 symbols per second. What is the throughput of the source?
**Answer**: $2^{3} = 8$

## Periodic continous signals
A *continous signal* $s(t):\mathbb{R} \rightarrow \mathbb{R}$ is **periodic** with period $T$ if:
					$s(t) = s(t + T) \forall T \in \mathbb{R}$
An example of periodic signal is $s(t) = sin(nt)$ and $s(t)=cos(nt)$ with period $T = \frac{2\pi}{n} (\forall N \in \mathbb{Z})$.
Periodic signals can be studied in the period $[0,T]$ since their behavior remains the same in all its domain of existence. For a **periodic signal** also holds the property:
	1$s(t+T) = s(t+2T) = s(t+nT) \forall t \in \mathbb{R}, n \in \mathbb{Z}$. 

Here an example of periodic continous signal:
$s(t)= cos(2t) + 5 \times sin(2t) - cos(5t) +4 \times sin(6t)$ 
which has period $T = 2\pi$.
![[Pasted image 20230505114537.png]]
By knowing the signal in a specific period $T$, we can use **periodic extension of aperiodic signals** to obtain the signal in the subsequent periods.  This can be applied to an **aperiodic signal** $s$ with ***support limited*** to the interval $[a,b)$ such thath $s(t) = 0 \forall t \notin [a,b)$. The ***periodic extension**** $s^{*}$ of $s$ is defined as:
		$s^{*}(t) = \sum^{\infty}_{n=-\infty}s(t-nT)$ 
where $T = b-a$, obtaining the following pictured extension:
![[Pasted image 20230505115013.png]]

## Transmission
Signals may have different nature, depending on the type of transmission channel: they can be *electromagnec, sound, optical, waves, etc*. 
At the source, a **transducer** converts a message into a signal, while at the destination another transducer converts a signal into a message (*an antenna is a tansduer for electromagnetic signals*). 
During the transmission from source to destination, the signal can be **distorted** or turbated by the **noises** that charaterize the specific *trasmissive medium*. The *distortion* of a signal is commo n when dealing with a signal composed by *different waves* that are attenuated during the transmission, obtaining at destination a different signal respect to the source one. 
A common metric for signal is **SNR - Signal to Noise ratio** express the index quality of a chanell, allowing to measure the intrinsic property of the transmissive medium, not of a message itself. 

The transmission process can be **analogic** or **digital** based on the type of signal to be transmitted. 
The *analog transission* use devices called **adaptors** that reduce the effect of noise and increase the efficiency, improving the quality of the signal. A **transducer** can be imagined as the *antenna* that emits the *electromagnetic waves* on the sender side or on the receiver-one used during signal reception, transforming it into an **electrical signal**. 
![[Pasted image 20230505120141.png]]
 
Differently, in the **digital transmission**, there is an exchange of *discrete (sequence of symbols) messages* and can be performed also over an *analog channel*, as pictured here:
![[Pasted image 20230505120531.png]]
The previous schema made use of the *analog channel* transmission component already described, using at both the transmission and reception phase a component called **modem**: it allows to perform *modulation* and *demodulation*, transforming the discrete signal in an *analog signal* (*modulation*) and, at receiver side, extracting from the received *analog signal* the correspondent *digital message* transmitted by the source. 

The error probability is computed based on probability of *noise/distortion* happened during the analog transmission phase, combined with error in the *demodulation phase*. An optimal solution is to provide a stable or predictable limit to this probabiltiy, allowing **channel encoding** to use also *redundancy mechanism* to correct errors in *transmission and demodulation phase*.


#### Sampling and quantization
Respect to the **digital transmission**, there are preliminar operations to be carried out both by the transmitter and receiver before starting transmitting the signal.
The **transmitter** first sample the *analog signal* at *discrete intervals* (*quantization operation*) by means of an **Analog to Digital Converter (ADC)**, allowing tge analog signal to be represented into a sequence of computable digital symbols.
At the **receiver** side, the received symbols must be converted again into an *analog signal* by means of a **Digital to Analog Converter (DAC)**.
![[Pasted image 20230505121537.png]]


## Fourier series
The cornestor of signal and system analysis are both the **Frequency Domain Analysis (FDA)** and the **Fourier transforms**.
The general expression for a *sinudoid* at frequency $\omega$ (*or frequency $f$ in Hertz*) is:
$x(t)=\alpha sin(\omega t + \varphi) = \alpha sin(2\pi ft + \varphi)$
We can combine the two sinudoids in:
$x(t) = sin(2\pi t) + sin(4\pi t)$

![[Pasted image 20230506094754.png]]

![[Pasted image 20230506094845.png]]

- $s_{0}(t)$: constant component (frequency 0)
- $s_{1}(t)$: fundamental (frequency $f_{0}= 1/T$)
- $s_{2}(t)$: second harmonic (frequency $f_{1}= 2/T=2f_{0}$)
- $s_{3}(t)$: third harmonic (frequency $f_{2}= 3/T=3f_{0}$ )
- $s_{4}(t)$: fourth harmonic (frequency $f_{1}= 4/T=4f_{0}$)
![[Pasted image 20230506095210.png]]



The key idea is to **decompose the signal** in its basic building block identified, expressing it as the **sum of an infinite number of continous functions**, oscillating at *different frequencies.* This represents a change of coordinates: from *time* domain to *frequency* domain. 
This set of continuous functions defines the ***base*** of decomposition: the Fourier series has a base represented by a set of functions $\varphi_{n}(t), n \in \mathbb{Z}$ in which the functions must be **orthogonal** (*as in the case of decomposition of vectors in a vector space*). 

Given a ***continuous*** singal $s(t):\mathbb{R} \rightarrow \mathbb{R}$, ***periodic*** in the interval $[-\pi, \pi]$, its Fourier series is define as:

$s(t) = \frac{1}{2}a_{0}+\sum_{n=1}^{\infty}a_{n}cos(nt)+b_{n}sin(nt)$
where:
- $a_{0} = \frac{1}{\pi} \int_{-\pi}^{\pi} s(t)dt$
- $a_{n}= \int_{-\pi}^{\pi} s(t)cos(nt)dt$
- $b_{n}=\int_{-\pi}^{\pi} s(t)sin(nt)dt$

![[Pasted image 20230506100244.png]]
It is rather complicate to assess the *conditions* under which an arbitrary $s(t)$ can be developed in a Fourier series. In particular **necessary conditions** are not known but the **sufficient conditions** given by the *Dirichlet theorem* are:
- $s(t)$ is *periodic*
- $s(t)$ is *piecewise continuous* (*finite number of discontinuities in the same period*)
Under these conditions the Fourier series of $s(t)$ *exists* and *converges* in $\mathbb{R}$.

The approximation improves with an high number of harmonics
