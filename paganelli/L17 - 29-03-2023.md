## Topology discovery and forwarding in SDNs
 Traditonally the **routing** function is distributed among the routers in a netowrk: in an **SDN controlled network** it make sense to centralize the routing unction within the SDN controller. The controller can develop a *consistent view* of the network state for calculating shortest paths and can implement application-aware routing policies.
 **Data plane switches** are relieved of the processing and storage burden associated with routing, leading to improved performances because **centralized routing** application performs two different functions:
 1. ***Link/Topology discovery***: routing function needs to be aware of the links between data plane switches. The topology discovery in OpenFlow is not fully standardized.
 2. ***Topology manager***: mantains the topology information for the network, computing routes in the network like the shortest path between two data plane nodes or between a data plane node and a host.

The **Topology Discovery** is implemented by exploiting two major *initial configuration* features of Openflow (*OF*) switches:
1. every OF switch has initially set the IP address and TCP port of a controller to establisha  connection as soon the device is turned on
2. switches have pre-defined behavior which support the implementation the support of the topology discover. It's supported by preinstalled flow rules to route directly to the controoler via a Packet-in message any message of the **Link Layer Discovery Protocol (LLDP)**.

##### LLDP - Link Layer Discovery Protocol
It's a **neighbor discovery protocol** of a single jump: it advertises its identity and capabilities and receive the same information from the adjacent switches. 
It's a vendor neutral protocol (*defined by IEEE*) ad operates at **layer 2** of OSI Model. 
It can be used also in traditional network and it's used by switches that support active configuration. In OpenFlow network, switches send the LLDP messages to discover the underluing topology by request the controller. 

LLDP uses *Ethernet* as its transport protocol, thus the Ethernet type for LLDP Is `0x88cc`. An LLDP frame contains the following fields:
- **Chassis ID (Type 1)**: contains the identifier of the switch that sends the LLDP packet. 
- **Port ID (Type 2)**: contains the identifier of the port through which the LLDP packet is sent. 
- **Time to Live (Type 3)**: time in seconds during which the information received in the LLDP packet is going to be valid. 
- **End of LLDPDU (Type 4)**: indicates the end of the payload in the LLDP frame
![[Pasted image 20230329163121.png]]

### Switch initialization
The initialization, for the pictured scenario, implies the need of the controller to understand the circular topology of the switches:
![[Pasted image 20230330100230.png]]
When the switch is initialized:
- Establish a connection with the controller
- The controller send a message *FEATURE REQUEST MESSAGE* to the switch
- The switch responds with a message *FEATURE REPLY MESSAGE
- It informs the controller of relevant parameter for the discovery of the links like the `Switch ID` and a list of `active ports` with their respective MAC associate, among other.
At the end of the initial handshake, the controller knows the exact
number of active ports on the OF switches **but** it doesn't know the physical connections between switches so it needs to build it by performing *Topology Discovery*

The **topology discovery** it's based on the information obtained from the initial handshake so the controller known the exact number of active ports:
1. The controller generates a **Packet-Out** message per active port on each switch discovered on the network and encapsulates a **LLDP packet** inside each generated message. 
2. When an OF switch receives a LLDP message sent by the controller, it **forwards the message by the appropriate Port** ID included in the message to adjacent switches 
3. Upon receiving the messages by a port that are not the controller port, **the adjacent switches encapsulate the packet within a Packet-In message addressed to the controller**. Meta-data is included in the message such as `Switch ID`, `Port ID` where the LLDP packet is received, among others 
4. When the packet comes back to the controller from an adjacent switch, the controller extracts this information, and then it knows a link exists between those switches

The controller at the end known the `Switch ID` and `Port ID` in the LLDP message and `Switch ID` and `Port ID` in metadata (*from which the switch received the LLDP message*).
This process is repear for every OF switch on the network: the entire process of discovery is performed periodically. In a network of $S$ switches interconnected by a set of links $L$, the total of packet-out message that the controller sends out to the network to  discover all existing link between OF switches with $P$ active port is:
$Total_{packet-out} = \sum_{i=1}^{S}{P_{i}}$

Let's clarify with an example, here pictured:
![[Pasted image 20230330100743.png]]
The controller send a **Packet-Out** to $S1$: the switch send the packet $S1-P1$ to host $A$ which it's not an LLDP device so did not answer, while send the LLDP packet $S1-P2$ to switch $S2$. The switch $S2$ add its metadata information like `Switch ID, Port ID` and send the **Packet-In** message to the controller that now knows that $S1$ and $S2$ are adjacent because the packet sended by $S2$ contains the information that the initial LLDP packet was received from $S1$ via port $1_{S2}$ .

The topology discovery itself does not able both the controller and the switches to identify which hosts are reachable.
The controller can learn that there is a **reachable host** in the network (*that is not an OF switch, so do not reply to OF packets*): the discovery is triggered by **unknown traffic entering** the controller's network domain either from an *attached host* or from a *neighboring router*. (*For unknown we mainly refer to not LLDP packets*).

For the **path computation**, initially flow tables on all switches are *empty*, butassume that Host table and Topology at thecontroller are fully populated with the necessary knowledge of the network and host.
Refer the scenario in following figure: when the switch receive a packet from the host `A` send the packet to the controller so it can compute the path and send the `Flow Mod` to update the flow table of the switches along the path from `A` to `B`. Based on the assumption that the controller already have all the kwnolesge about the host table and topology, the $S1$ and $S2$ flow table are updated as in figure.
![[host-reachability.png]]

These Flow Mod packets (*both from the controller to $S2$ and $S1$*) will be sent to each switch in **reverse order**, from the switch closest to the destination and so on until the source switch: the rationale behind this order is to **avoid the scenario in which multiple switches trigger the path computation** while there is the flow table updating operation ongoing by the controller. 

All future packets from this flow will match directly at each switch and no
longer need to take a trip up to the controller.

##### Exercise
Describe the source, destination and semantics of the following OF messages: 
- packet-in 
- packet-out 
- FlowMod:

## Application of SDN
The early adopters were companies that manages datacenter, for several use cases:
- Switching fabrics - SDN within data center: from black box to white box switches, server interconnected by software
- Virtual networks - SDN to setup, manage and shutdown
- Wide area networks - as a traffic engineering methods, optimizing the traffic between different datacenters (*e.g. Google WAN for inter-datacenter traffic*).

Here we analyze the **B4 - Google Use case** of SDN inside Google Datacenters.

### Google WAN
There are two separate backbones:
- **B2**: carries internet facing traffic. It have a growing traffic than the internet.
- **B4**: carries inter-datacenter traffic (*sort of horizontal traffic*). It have more traffic than B2 that also growsa faster than B2
![[Pasted image 20230330103422.png]]
Google have its own globally deployed WAN, private and connected to all its datacenter.
Data center are deployed across the world and need to ensure **multi-tenancy availability zone and redundancy** to guarantee availability and reliability of its service. The content are served based on geographic locality and replicated for fault tolerance among few dozens of datacenter around the world.
![[Pasted image 20230330103621.png]]

This network it's not on the public internet and must be cost-effective for high volume of traffic, performing load balancing among the DC.

Three main type of flow types are consired:
1. **User data copies** to remote data center for availability/reliability (*lowest volume, most latency intesnvie, highest priority*)
2. **Remote storage access** for computation over distributed data sources
3. Large-Scale data push **syncrhonizing state across multiple data center** (*highest volume, least latency intensive, lower priority*)

The main problem in traditional WAN routing is that all the flows are treated with the same behavior (*so there is no QoS*) , all the links are used on average with 30% to 40% with some peaks, downgrading overall performance in a small timeframe.

The **backbone B4** has been built considering some requirements:
- *Elastic bandwidth demand*: applications can tolerate periodic failures or temporary bandwidth reductions
- *Moderate number of sites*: few dozens of sites, no need for large routing tables
- *End application protocol*: Google has the control of the application so can optimize the schedule of the data transfer, using a fine-grained application protocol. This avoid the need for link provsioning. 
- *Cost sensivity*: private intercontinental links cost, so it needs to be used at its full capacity

The main *design decisions* applied on B4 backbone are related to:
1. **Customize HW and SW**: customize routing and monitoring protocols to requirements with a rapid custom protocol development
2. **Low cost switching hardware**: edge application control limits need for large buffers but the limited number of sites avoid the need to manage large forwarding tables
3. **Centralize traffic engineering (TE)** : allows more optimal and transfer traffic engineering than distributed control routing. It allows also to share bandwidth among competing applications. Usually the **TE server** is an application that works on top on the SDN controller.
4. **Increase link utilization**: allows an efficient use of expensive long haul transport. 
With **Traffic Engineering** usually we refer as a set of techniques to
optimize and control traffic flows ina telecommunication network in
order to ensure a maximum throughput and a sufficient QoS level.

#### B4 Architecture Overview
The BA architecture overview is here pictured:
![[Pasted image 20230329172844.png]]
Each site have an *OFA - OpenFlow Agent* while the *site controller* it's based on a distributed *NCS - Network Controller System* to avoid a single point of failure.
The *switch hardware* forwards traffic without using any complex control software and builded by using Google custom design with commodity sillicon.
The **control layer** is composed by *NCS* hosting both  OpenFlow controllers (*OFC*) and Network Control Application *(NCAs)*: the OFCs mantain network state based on NCA directives and switches events but also instruct the switches to set forwarding table entries based on this changing network state.

The **global layer** is composed by a logically centralized applications like the *SDN Gateway* and a *Central TE Server*: the SDN gateway abstracts details of OpenFLow and switches hardware from the central TE server. Each B4 site consits of multiple switches with potentially hundreds of individual ports. 

Each cluster contains a set of BGP router that peer with B4 switches at each WAN site: the rationale behind this choice was made due to the familiarity of the operator with the BGP protocol and mantain protocols retro-compatiblity, enabling also a gradual rollout. 

The **traffic engineering** components in the *global layer* are pictured here:
![[Pasted image 20230330110124.png]]

The figure shows an overview of our **TE architecture**. The TE Server operates over the following state:

- The **Network Topology** graph represents sites as vertices and site to site connectivity as edges. The *SDN Gateway* consolidates topology events from multiple sites and individual switches to TE. TE aggregates trunks to compute site-site edges. This abstraction significantly reduces the size of the graph input to the *TE Optimization Algorithm*.

- **Flow Group (FG)**: For scalability, TE cannot operate at the granularity of individual applications. Therefore, we aggregate applications to a Flow Group defined as `{source site, dest site, QoS}` tuple. 
- A **Tunnel (T)** represents a site-level path in the network, e.g., a sequence of sites (`A → B → C`). B4 implements tunnels using IP in IP encapsulation. 
- A **Tunnel Group (TG)** maps FGs to a set of tunnels and corresponding weights. The weight specifies the fraction of FG traffic to be forwarded along each tunnel.

TE Server outputs the Tunnel Groups and, by reference, Tunnels and Flow Groups to the SDN Gateway. Thee Gateway forwards these Tunnels and Flow Groups to OFCs that in turn install them in switches using OpenFlow.
Another main component is the **Bandwidth Enforcer**:  to capture relative priority, we associate a *bandwidth function* with every application, effectively a contract between an application and B4. This function specifies the bandwidth allocation to an application given the flow’s *relative priority* on an arbitrary, dimensionless scale, which we call its **fair share**. 
We derive these functions from administrator-specified *static weights* (the slope of the function) specifying relative application priority. In this example, $App1$, $App2$, and $App3$ have weights $10, 1$, and $0.5$, respectively. Bandwidth functions are configured, measured and provided to TE via Bandwidth Enforcer.
![[Pasted image 20230330105706.png]]

#### Improvements results
OpenFlow has helped Google improving B4 backbone (*referece 1*): 
- Utilization near 100% for elastic loads
- Mirror production events for testing on virtualized switches
- Reduced complexity
- Reduced costs
With follow-up improvements (*referece 2*):
- hierarchical topology
- decentralized TE algorithms
![[Pasted image 20230330105902.png]]
For further information about B4:
1. *Sushant Jain et al. “B4: experience with a globally-deployed software defined wan”. In Proc. of the ACM SIGCOMM 2013 ACM, New York, NY, USA, 3-14*
2. *Hong CYY et al.,. B4 and after: managing hierarchy, partitioning, and asymmetry for availability and scale in google's software-defined WAN. In Proc. Proc. of the ACM SIGCOMM 2018 Aug 7 (pp. 74-87)*.

